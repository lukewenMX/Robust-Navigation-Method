# Sidewalk Navigation with Sparse Semantic Point Cloud

![](./images/Overview.png)

- ```pc_seg``` directory provides scripts to 
  - train different variants of PointNet for point segmentation (Original PointNet, light version of PointNet, both of them are only accounting for the x,y,z information of point cloud, and our model)
  - run the navigation test on robot with different types of sensor input (image, point cloud and random controller etc.) and pretrained model



- ```prepare_data``` directory provides scripts to
  - extract different types of data (images, pointclouds etc.) from rosbag
  - manually split a whole dataset into train and test part and generate list file respectively
  - autolabel the sparse point cloud with the semantic cue from images, which is generated with an off-the-shelf image segmentation neural network.

- The dataset we used to train and test the proposed method can be downloaded through [nanyang_link at NTU](https://drive.google.com/file/d/19jlJjrdp_5CfxelZEjouCHW9srt6EP9C/view?usp=sharing), which are generated from recorded rosbag files [rosbag] (https://www.dropbox.com/home/rosbags, please contact me for more rosbags via mingxing001@e.ntu.edu.sg) and the platform is Husky A200 associatedd with Velodyne PUCK and ZED RGB-D camera for collecting data in nanyang link route from NTU, Singapore.

- ```range_image``` directory illustrates scripts to
  - train the Reactive Navigation network based on CNN-LSTM structure, where spatio-temporal information are maintained as much as possible. And the semantic point cloud has been transferred firstly to semantic range image before fed to network.

## Prerequesite 
- Ubuntu 20.04 with noetic ROS (since the default version in noetic is python3, and the code works properly with python3. You can also install anoconda in previsions of ros for running it)
- Anoconda 3.0
- pytorch

## Instructions
### 1. Autolabelling the semantic point cloud 
Launch a ros master
- ```roscore``` 

Transfer the compressed images to raw ones
- ```rosrun image_transport republish compressed in:=/camera/left/image_raw raw out:=/camera/left/image_raw```  

Set the rosparam
- ``rosparam set use_sim_time true``

Generate the semantic point cloud and store the corresponding associated data (image, semantic image, raw PC, semantic PC, twist, etc per what you required)
- ``python cam_lidar_fusion.py``

Republish the twist massage to odom message
- ``python ./republisher.py``

Replay the rosbag which you recorded, we have open sourced the generated dataset for sidewalk scenarios [nanyang_link at NTU](https://drive.google.com/file/d/19jlJjrdp_5CfxelZEjouCHW9srt6EP9C/view?usp=sharing)
- ``rosbag play  --clock *.bag``

### 2. Train pointnet variants network: SegPCn
- ```python ./pc_seg/scripts/point_train.py --mode train --model_name pointnet```

### 3. Train the RNn
- ```python ./range_image/train.py```


## Results
1. Qualitative and Quantitative Performance of Semantic point cloud network
![Segmentation results](./images/segmentation.png)
![Qualitative results](./images/qualitative%20performance.png)
2. Qualitative amd Quantitative performance of RNn, evaluated in sidewalk scenarios with two baselinesï¼š CNN & Random![navigation performance](./images/Navigation%20performance.png)
![navigation screenshot](./images/sidewalk_navigation.png)


## Citation
Please refer to our IROS 2022 paper for more details, which would be published soon, if you use any of the code.
```
@INPROCEEDINGS{IROS2022-WEN,
  author={Mingxing Wen, Yunxiang Dai, Tairan Chen, Chunyang Zhao, Jun Zhang and Danwei Wang},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title={A Robust Sidewalk Navigation Method for Mobile Robots Based on Sparse Semantic Point Cloud},
  year={to be published, 2022},
  organization={IEEE}

```


